---
title: "🔗 MML Chapter 8: 모델과 데이터의 만남 (When Models Meet Data)"
date: 2025-04-07
tags: [MML, 머신러닝 모델, 경험위험, 최대우도, 베이지안, 그래프모델]
categories: [Study, Mathematics for Machine Learning]
math: true
---

# 📘 MML Study - Chapter 8: When Models Meet Data 요약

> 이 장에서는 머신러닝의 3대 구성요소인 **데이터, 모델, 학습**을 수학적 언어로 정립하며, 실제 머신러닝 알고리즘 설계로 이어지는 연결 고리를 다룬다.  
> 특히 **경험 위험 최소화, 최대우도 추정, 베이지안 추론, 확률 그래프 모델, 모델 선택** 등 머신러닝의 실천적 근거가 되는 핵심 주제를 폭넓게 다룬다.

---

## 📚 목차

- 8.1 [데이터, 모델, 학습 개념 정리](#8.1)
- 8.2 [경험 위험 최소화 (Empirical Risk Minimization)](#8.2)
- 8.3 [최대우도 추정 (Maximum Likelihood)](#8.3)
- 8.4 [확률적 모델링과 추론](#8.4)
- 8.5 [확률 그래프 모델 (Graphical Models)](#8.5)
- 8.6 [모델 선택 (Model Selection)](#8.6)

---

## 8.1 데이터, 모델, 학습 개념 정리 <a name="8.1"/>

- **What:** 머신러닝은 데이터를 입력으로 받아 모델을 구성하고, 학습을 통해 예측 성능을 높이는 일련의 과정이다.

- **Why:** 실제 문제를 해결하기 위해선 학습이 단순한 수치 최적화를 넘어서야 하며, 모델 구조와 데이터 표현이 핵심이다.

- **When:**  
  - 벡터 표현된 데이터에 대한 모델 학습  
  - 일반화 가능한 예측기 설계  
  - 성능 기준 기반의 평가

- **How:**  
  - 예측기: $f: \mathbb{R}^D \rightarrow \mathbb{R}$ 또는 $f(x) = \theta^\top x + \theta_0$  
  - 모델: 함수 기반 또는 확률분포 기반  
  - 학습의 3단계: 예측 → 학습 → 모델 선택

> 📌 **콜아웃**  
> 학습이란 **적절한 모델을 찾는 과정**이며, 이 과정에는 수학적 최적화, 통계, 확률, 데이터 표현이 모두 얽혀 있다

---

## 8.2 경험 위험 최소화 (Empirical Risk Minimization) <a name="8.2"/>

- **What:**  
  주어진 데이터에서 손실 함수(loss)를 최소화하여 파라미터를 추정하는 학습 프레임워크

- **Why:**  
  훈련 데이터를 통해 직접 학습 문제를 수치 최적화 문제로 환원시킬 수 있음

- **When:**  
  - 비확률적 모델  
  - 선형 회귀, 서포트 벡터 머신  
  - 딥러닝에서의 손실 기반 학습

- **How:**  
  $$
  \min_\theta \frac{1}{N} \sum_{n=1}^N \mathcal{L}(f(x_n;\theta), y_n)
  $$  
  - 일반화 오류를 줄이기 위해 정규화 항 추가 가능: $\lambda R(\theta)$

> 🎯 **콜아웃**  
> 경험 위험은 모델의 **적합도**, 정규화는 모델의 **복잡도 제어**를 뜻한다. 둘의 균형이 핵심이다

---

## 8.3 최대우도 추정 (Maximum Likelihood Estimation) <a name="8.3"/>

- **What:**  
  데이터가 특정 분포에서 생성되었다고 가정할 때, 그 분포의 파라미터를 가장 잘 설명하는 값을 찾는 방법

- **Why:**  
  확률적 모델의 학습에서 가장 널리 사용되며, 베이지안 추론의 출발점이 됨

- **When:**  
  - 통계적 회귀  
  - 가우시안 모델링  
  - 분류기 확률 예측

- **How:**  
  $$
  \hat{\theta}_{MLE} = \arg\max_\theta \prod_{n=1}^N p(y_n | x_n, \theta)
  $$  
  또는 로그우도(log-likelihood) 최대화

> 🧮 **콜아웃**  
> MLE는 **최소제곱법의 일반화**이며, 수학적으로는 로그우도를 **오차 함수로 해석**하는 형태로 연결된다

---

## 8.4 확률적 모델링과 추론 <a name="8.4"/>

- **What:**  
  데이터와 파라미터를 모두 확률 변수로 보고, 전체를 **결합분포(joint distribution)**로 기술하는 모델링 방식

- **Why:**  
  불확실성을 정량화하고, **사전 지식 통합**, **예측의 불확실성 제공**이 가능함

- **When:**  
  - 베이지안 추론  
  - 샘플 기반 예측  
  - 확률적 결정 및 의사결정

- **How:**  
  - 사후 분포:  
    $$
    p(\theta | X) = \frac{p(X|\theta)p(\theta)}{p(X)}
    $$  
  - 사후 예측:  
    $$
    p(y^*|x^*, X) = \int p(y^*|x^*, \theta)p(\theta|X)d\theta
    $$

> 🔍 **콜아웃**  
> 베이지안 추론은 **모델의 불확실성을 유지**하며, 더 강건한 예측이 가능하게 해준다

---

## 8.5 확률 그래프 모델 (Graphical Models) <a name="8.5"/>

- **What:**  
  확률변수 간 관계를 **그래프 구조**로 표현하여 결합분포를 효율적으로 인코딩하는 모델

- **Why:**  
  구조적 가정을 통해 **계산 효율성**을 확보하고, **조건부 독립성**을 시각화할 수 있음

- **When:**  
  - 베이지안 네트워크 (DAG)  
  - 마르코프 랜덤 필드  
  - 인과모형 설계

- **How:**  
  - 방향 그래프: 인과 관계 표현  
  - 인접 조건: D-separation 기반 조건부 독립성 표현  
  - 예: $X \perp Z \mid Y \Rightarrow p(x, z | y) = p(x|y)p(z|y)$

> 📊 **콜아웃**  
> 그래프 모델은 **수식 대신 구조로 모델링**하며, 설계와 해석에 강력한 프레임워크를 제공한다

---

## 8.6 모델 선택 (Model Selection) <a name="8.6"/>

- **What:**  
  여러 후보 모델 중에서 일반화 성능이 가장 좋은 모델을 선택하는 절차

- **Why:**  
  과적합(overfitting)과 과소적합(underfitting)을 피하면서 일반화 가능한 모델을 찾는 것이 핵심

- **When:**  
  - 하이퍼파라미터 튜닝  
  - 커널/차원 수/모델 아키텍처 결정  
  - AIC/BIC 기반 비교

- **How:**  
  - 교차검증 (Cross-validation)  
  - 정보 기준:  
    $$
    \text{AIC} = \log p(x|\theta) - M,\quad
    \text{BIC} = \log p(x|\theta) - \frac{1}{2}M\log N
    $$  
    ($M$: 파라미터 수, $N$: 데이터 수)

> 🧠 **콜아웃**  
> 모델 선택은 수학적으로는 **모델 복잡도에 대한 벌점(penalty)**과 **우도(log-likelihood)** 간의 균형이다

---

✅ 다음 장인 **[Chapter 9: 선형 회귀 (Linear Regression)]**에서는 머신러닝의 대표적인 지도학습 기법으로 **선형 모델 기반 예측, 최대우도, 정규화 회귀, 베이지안 회귀** 등을 다룬다.  
📈 데이터와 예측 함수의 관계를 수학적으로 모델링해보자.
