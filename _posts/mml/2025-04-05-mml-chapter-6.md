---
title: "🎲 MML Chapter 6: 확률과 분포 (Probability and Distributions)"
date: 2025-04-05
tags: [MML, 확률, 분포, 통계, 베이즈, 머신러닝수학]
categories: [Study, Mathematics for Machine Learning]
math: true
---

# 📘 MML Study - Chapter 6: Probability and Distributions 요약

> 본 장에서는 머신러닝에서 **불확실성의 수학적 표현**을 위한 도구인 확률 이론의 핵심 요소들을 소개한다.  
> 확률 공간, 랜덤 변수, 이산/연속 분포, 조건부 확률, 베이즈 정리, 다양한 대표 분포 등을 다루며, **확률적 모델링의 기초**를 다진다.

---

## 📚 목차

- 6.1 [확률 공간의 구성 (Probability Space)](#6.1)
- 6.2 [이산 및 연속 확률분포 (Discrete and Continuous Distributions)](#6.2)
- 6.3 [합/곱 규칙 및 베이즈 정리 (Sum Rule, Product Rule, Bayes)](#6.3)
- 6.4 [기댓값과 분산 (Expectation and Variance)](#6.4)
- 6.5 [가우시안 분포 (Gaussian Distribution)](#6.5)
- 6.6 [결합 분포, 조건부 분포, 베타 분포 및 켤레 사전분포](#6.6)
- 6.7 [변수 변환과 역변환 기법 (Change of Variables)](#6.7)
- 6.8 [참고자료 (Further Reading)](#6.8)

---

## 6.1 확률 공간의 구성 <a name="6.1"/>

- **정의 (What):**  
  확률 공간은 실험의 모든 가능한 결과(샘플 공간 $\Omega$), 사건의 집합 $\mathcal{A}$, 확률 측도 $P$로 구성된다.

- **의의 (Why):**  
  머신러닝에서 데이터, 모델, 예측의 불확실성을 수학적으로 설명하기 위해 필수적이다.

- **활용 시점 (When):**  
  - 데이터 생성 과정 모델링  
  - 불확실성 하의 추론  
  - 베이지안 학습의 기반

- **방법 (How):**  
  - $(\Omega, \mathcal{A}, P)$로 구성  
  - 랜덤 변수: $X: \Omega \rightarrow T$  
  - 확률 분포: $P_X(S) = P(X^{-1}(S))$

> 🧠 **콜아웃**  
> 확률 이론은 논리적 추론의 확장으로 볼 수 있으며, **자동화된 의사결정** 시스템의 핵심이다

---

## 6.2 이산 및 연속 확률분포 <a name="6.2"/>

- **정의 (What):**  
  이산 확률분포는 특정 값에 대한 확률(PMF), 연속 확률분포는 구간에 대한 확률(PDF + CDF)로 정의된다.

- **의의 (Why):**  
  입력 데이터가 이산/연속 여부에 따라 확률모델의 형태가 결정된다.

- **활용 시점 (When):**  
  - 분류/회귀 문제 모델링  
  - 라벨 예측 vs 연속값 예측  
  - 샘플링 기반 기법

- **방법 (How):**  
  - PMF: $P(X = x)$  
  - PDF: $p(x) \geq 0$, $\int p(x)dx = 1$  
  - CDF: $F_X(x) = P(X \leq x)$

> 🎯 **콜아웃**  
> 연속 변수에 대해 $P(X = x) = 0$이라는 사실은 직관적으로 반직관적일 수 있으나, 이는 측도의 개념으로 이해된다

---

## 6.3 합/곱 규칙 및 베이즈 정리 <a name="6.3"/>

- **정의 (What):**  
  - 합 규칙: $p(x) = \sum_y p(x, y)$ 또는 $\int p(x, y)dy$  
  - 곱 규칙: $p(x, y) = p(x)p(y|x)$  
  - 베이즈 정리: $p(x|y) = \frac{p(y|x)p(x)}{p(y)}$

- **의의 (Why):**  
  확률 이론의 두 기본 규칙이며, 베이즈 정리는 관측을 통한 믿음 업데이트를 가능하게 한다.

- **활용 시점 (When):**  
  - 조건부 추론  
  - 베이즈 분류기, 나이브 베이즈  
  - 확률적 그래프 모델

- **방법 (How):**  
  주어진 결합 확률 또는 조건부 확률에서 미지 확률을 계산

> 🔁 **콜아웃**  
> 베이즈 정리는 **사전(이전 지식) + 우도(관측) → 사후(업데이트된 믿음)** 구조를 가진다

---

## 6.4 기댓값과 분산 <a name="6.4"/>

- **정의 (What):**  
  - 기댓값: $E[X] = \sum xP(x)$ 또는 $\int xp(x)dx$  
  - 분산: $Var(X) = E[(X - E[X])^2]$

- **의의 (Why):**  
  평균적 동작 및 불확실성의 범위를 정량적으로 표현할 수 있다.

- **활용 시점 (When):**  
  - 평균 손실 계산  
  - 분포 요약  
  - 정규분포 정의

- **방법 (How):**  
  - 분산의 성질: $Var(X) = E[X^2] - (E[X])^2$  
  - 공분산 및 공분산 행렬 확장

> 📊 **콜아웃**  
> 기댓값은 분포의 중심, 분산은 분포의 퍼짐을 나타내며, **모델의 안정성 분석**에 필수이다.

---

## 6.5 가우시안 분포 <a name="6.5"/>

- **정의 (What):**  
  연속 확률 분포 중 하나로, 중심 극한 정리에 의해 자연스럽게 나타나며 수학적으로 가장 다루기 쉬운 분포

- **의의 (Why):**  
  회귀, 분류, 강화학습, 신호처리 등 다양한 분야에서 중심적 역할

- **활용 시점 (When):**  
  - 회귀 해 추정  
  - 잡음 모델링  
  - 차원축소 (PCA)

- **방법 (How):**  
  - 단변량:  
    $$
    p(x|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
    $$  
  - 다변량: 공분산 행렬 $\Sigma$, 평균 벡터 $\mu$ 활용

> 🧠 **콜아웃**  
> 대부분의 머신러닝 분포 모델은 **가우시안에 기반하거나, 근사된다**

---

## 6.6 결합 분포, 조건부 분포, 베타 분포 및 켤레 사전분포 <a name="6.6"/>

- **정의 (What):**  
  - 결합: $p(x, y)$  
  - 조건부: $p(x|y)$  
  - 베타: $p(\mu|\alpha, \beta) \propto \mu^{\alpha-1}(1 - \mu)^{\beta - 1}$

- **의의 (Why):**  
  확률적 추론, 베이즈 추정, MCMC 샘플링 등에 핵심적으로 사용됨

- **활용 시점 (When):**  
  - 베르누이 분포의 사전으로 베타 분포  
  - 베이지안 업데이트  
  - 추론 모델 구축

- **방법 (How):**  
  - conjugate prior: 사후 분포도 같은 형태 유지  
  - 베타-베르누이, 감마-푸아송 등 조합 사용

> 🧩 **콜아웃**  
> 켤레 사전분포는 **수학적 편의성과 해석력**을 동시에 제공한다

---

## 6.7 변수 변환과 역변환 기법 <a name="6.7"/>

- **정의 (What):**  
  랜덤 변수의 함수로 새로운 분포를 생성하는 기법. 예: $Y = g(X)$

- **의의 (Why):**  
  복잡한 분포를 단순한 분포에서 유도할 수 있으며, 정규화 흐름 등에서 활용

- **활용 시점 (When):**  
  - 비선형 특성 모델링  
  - 샘플링 기반 추론  
  - 정규화 흐름 (normalizing flows)

- **방법 (How):**  
  - $p_Y(y) = p_X(x) \left| \frac{dx}{dy} \right|$ (단변수)  
  - 야코비안 행렬 사용 (다변량)

> 🔄 **콜아웃**  
> 복잡한 분포도 **변환 함수와 단순한 분포**의 조합으로 모델링할 수 있다.

---

## 6.8 참고자료 (Further Reading) <a name="6.8"/>

- 📘 **서적**
  - *Information Theory, Inference and Learning Algorithms* — David MacKay  
  - *Pattern Recognition and Machine Learning* — C. Bishop  
  - *Bayesian Reasoning and Machine Learning* — David Barber

- 🎥 **강의**
  - MIT OCW — Probability and Statistics  
  - YouTube: 3Blue1Brown "Bayes Theorem" 시리즈

---

✅ 다음 장인 **[Chapter 7: 최적화 (Optimization)]**에서는 머신러닝 모델의 **학습 과정과 파라미터 조정의 수학적 근거**가 되는 최적화 이론을 다룬다.  
⚙️ 경사하강법, 뉴턴 방법, 볼록 최적화 등 머신러닝의 엔진을 구성하는 알고리즘을 학습해보자.
