---
title: "📊 MML Chapter 11: 밀도 추정 (Density Estimation)"
date: 2025-04-10
tags: [MML, 밀도추정, GMM, EM알고리즘, 확률모델, 머신러닝수학]
categories: [Study, Mathematics for Machine Learning]
math: true
---

# 📘 MML Study - Chapter 11: Density Estimation 요약

> 이 장에서는 머신러닝의 세 번째 축인 **밀도 추정(density estimation)**을 다룬다.  
> 데이터가 생성된 기반 분포를 추정하여 데이터의 구조를 이해하고, 이를 통해 **표현, 이상탐지, 샘플링** 등을 가능하게 한다.  
> 특히 **가우시안 혼합 모델(GMM)**과 **기대 최대화 알고리즘(EM algorithm)**의 수학적 유도를 중심으로 구성된다.

---

## 📚 목차

- 11.1 [밀도 추정의 필요성과 접근법](#11.1)
- 11.2 [가우시안 혼합 모델 (GMM)](#11.2)
- 11.3 [기대 최대화 알고리즘 (EM Algorithm)](#11.3)
- 11.4 [모델 설정, 초기화, 예시](#11.4)
- 11.5 [비판과 확장: 베이즈적 접근과 KDE](#11.5)
- 11.6 [참고자료 (Further Reading)](#11.6)

---

## 11.1 밀도 추정의 필요성과 접근법 <a name="11.1"/>

- **What:**  
  주어진 데이터 집합을 기반으로 **확률 밀도 함수** $p(x)$를 추정하는 문제

- **Why:**  
  데이터 생성 과정을 이해하고, 샘플링, 이상탐지, 불확실성 추정 등에 사용

- **When:**  
  - 라벨 없는 데이터 구조 분석  
  - 데이터 생성 모델링  
  - 베이지안 추론 기반 강화

- **How:**  
  - 파라메트릭 방식: Gaussian, Exponential 등  
  - 비파라메트릭 방식: 히스토그램, 커널 밀도 추정 (KDE)

> 📦 **콜아웃**  
> 밀도 추정은 **지도학습 없이 데이터의 본질을 이해하는 방법**이다.

---

## 11.2 가우시안 혼합 모델 (GMM) <a name="11.2"/>

- **What:**  
  여러 개의 가우시안 분포를 조합하여 다봉(multi-modal) 구조를 가진 분포를 모델링

- **Why:**  
  단일 가우시안으로는 표현이 어려운 복잡한 데이터 구조를 유연하게 추정 가능

- **When:**  
  - 군집분석  
  - 이상탐지  
  - 이미지/신호 모델링

- **How:**  
  $$
  p(x) = \sum_{k=1}^K \pi_k \mathcal{N}(x | \mu_k, \Sigma_k)
  $$
  - $\pi_k$: 혼합 비율, $\sum \pi_k = 1$  
  - $\mu_k, \Sigma_k$: 각 컴포넌트의 평균과 공분산

> 🎯 **콜아웃**  
> GMM은 **하나의 분포로는 설명이 어려운 데이터 집합**에 적합한 유연한 모델이다.

---

## 11.3 기대 최대화 알고리즘 (EM Algorithm) <a name="11.3"/>

- **What:**  
  GMM의 파라미터 $\theta = \{\pi_k, \mu_k, \Sigma_k\}$를 최대우도 기반으로 추정하기 위한 반복 알고리즘

- **Why:**  
  로그-우도의 최대화가 폐쇄형 해를 가지지 않기 때문에 **간접적 추정 방식**이 필요

- **When:**  
  - 혼합 모델 학습  
  - 잠재 변수(latent variable) 기반 모델  
  - 강화학습의 숨은 상태 추정

- **How:**  
  1. 초기화: 파라미터 설정  
  2. E-step: 책임도 계산  
     $$
     r_{nk} = \frac{\pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k)}{\sum_j \pi_j \mathcal{N}(x_n|\mu_j, \Sigma_j)}
     $$
  3. M-step: 파라미터 업데이트  
     $$
     \mu_k = \frac{1}{N_k} \sum_n r_{nk} x_n,\quad \Sigma_k = \cdots,\quad \pi_k = \frac{N_k}{N}
     $$

> 🔄 **콜아웃**  
> EM은 **E와 M을 번갈아 반복하며 수렴**, 매 반복마다 로그-우도가 증가하는 성질을 가진다.

---

## 11.4 모델 설정, 초기화, 예시 <a name="11.4"/>

- **단일 가우시안 vs GMM**: 단일 가우시안은 평균과 분산으로 충분하지만 GMM은 다수의 모드 표현 가능  
- **초기화 중요성**: 파라미터 초기값에 따라 수렴 결과 달라짐  
- **예시**: $K=3$, 초기 가우시안 세 개로 시작해 반복적으로 피팅 진행

> 🧪 **콜아웃**  
> 초기화와 클러스터 수 $K$는 결과에 직접적인 영향을 미치며, 모델 선택 기준이 필요하다.

---

## 11.5 비판과 확장: 베이즈적 접근과 KDE <a name="11.5"/>

- **MLE 한계:**  
  - 과적합 위험 (특히 $\Sigma \to 0$인 경우)  
  - 불확실성 표현 부족

- **확장 방안:**  
  - **베이즈 추정**: 파라미터에 사전 분포를 부여하여 사후분포를 추론  
  - **변분 추론, MCMC** 등을 통해 근사 가능  
  - **KDE (Kernel Density Estimation)**: 비모수적 방식, 부드럽고 유연한 밀도 추정

> 🔍 **콜아웃**  
> GMM의 베이즈 추정은 **모델 비교, 군집 수 추론, 불확실성 정량화**에 기여할 수 있다.

---

## 11.6 참고자료 (Further Reading) <a name="11.6"/>

- 📘 **서적**
  - *Pattern Recognition and Machine Learning* — Christopher Bishop  
  - *Bayesian Reasoning and Machine Learning* — David Barber  

- 🎥 **강의**
  - YouTube: StatQuest "Gaussian Mixture Models and EM"  
  - MIT OpenCourseWare — Unsupervised Learning

---

✅ 다음 장인 **[Chapter 12: 분류 (Classification)]**에서는 지도학습의 또 다른 축인 **클래스 예측 모델링**을 다룬다.  
🏷️ 확률적 분류기, 결정경계, SVM, 베이지안 분류 등 머신러닝 핵심 개념을 정리해보자.
