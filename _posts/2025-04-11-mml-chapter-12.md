---
title: "🏷️ MML Chapter 12: 분류 (Classification)"
date: 2025-04-11
tags: [MML, 분류, SVM, 결정경계, 서포트벡터머신, 머신러닝수학]
categories: [Study, Mathematics for Machine Learning]
math: true
---

# 📘 MML Study - Chapter 12: Classification 요약

> 본 장은 머신러닝의 네 번째 축인 **분류(classification)** 문제를 다룬다.  
> 특히 **서포트 벡터 머신(SVM)**을 중심으로 이진 분류 문제를 수학적으로 정식화하고, 기하학적 직관 및 최적화 이론을 결합하여 실용적인 분류기를 설계하는 방법을 다룬다.

---

## 📚 목차

- 12.1 [이진 분류 문제 정의](#12.1)
- 12.2 [선형 분리와 마진 (Primal SVM)](#12.2)
- 12.3 [쌍대 문제와 커널 기법 (Dual SVM & Kernels)](#12.3)
- 12.4 [최적화 및 수치적 구현](#12.4)
- 12.5 [확률적 해석과 기타 분류기 비교](#12.5)
- 12.6 [참고자료 (Further Reading)](#12.6)

---

## 12.1 이진 분류 문제 정의 <a name="12.1"/>

- **What:**  
  두 클래스 $\{+1, -1\}$ 중 하나로 입력 벡터 $x \in \mathbb{R}^D$를 분류하는 문제

- **Why:**  
  스팸 필터링, 암 진단, 품질 분류 등 다양한 실제 응용에 필수

- **When:**  
  - 명확한 레이블이 있는 지도학습  
  - 결정 경계가 필요할 때  
  - 성능 평가 지표(정확도 등)가 명확할 때

- **How:**  
  분류기 함수:  
  $$
  f(x) = \text{sign}(\langle w, x \rangle + b)
  $$  
  하이퍼플레인 $\langle w, x \rangle + b = 0$을 기준으로 공간을 나눔

> 📏 **콜아웃**  
> 분류는 **공간 분할**의 문제이며, 그 구조를 **기하학적 하이퍼플레인**으로 표현할 수 있다.

---

## 12.2 선형 분리와 마진 (Primal SVM) <a name="12.2"/>

- **What:**  
  최대 마진 분류기를 설계하기 위한 최적화 문제 정의

- **Why:**  
  마진을 최대화하면 일반화 성능이 향상됨

- **When:**  
  - 라벨이 선형적으로 분리 가능한 경우  
  - 과적합 방지  
  - 구조적 위험 최소화 (SRM)

- **How:**  
  $$
  \min_{w,b} \frac{1}{2} \|w\|^2 \quad \text{s.t. } y_n(\langle w, x_n \rangle + b) \ge 1
  $$  
  - 힌지 손실 사용 가능: $L(y, f(x)) = \max(0, 1 - y f(x))$

> 🧮 **콜아웃**  
> SVM은 마진을 최대화하면서 동시에 **오류를 최소화하는 최적화 문제**로 표현된다.

---

## 12.3 쌍대 문제와 커널 기법 <a name="12.3"/>

- **What:**  
  제약 최적화 문제를 라그랑주 쌍대 문제로 전환하여 해결

- **Why:**  
  커널 기법을 통해 선형 분류를 **비선형 분류**로 확장 가능

- **When:**  
  - 입력 데이터가 선형 분리 불가능한 경우  
  - 고차원 특징 공간 필요 시  
  - 커널 SVM 사용 시

- **How:**  
  쌍대 문제:  
  $$
  \max_\alpha \sum_n \alpha_n - \frac{1}{2} \sum_{n,m} \alpha_n \alpha_m y_n y_m \langle x_n, x_m \rangle
  $$  
  커널 적용: $\langle x_n, x_m \rangle \rightarrow k(x_n, x_m)$

> 🔁 **콜아웃**  
> 커널 SVM은 **명시적 고차원 맵핑 없이 비선형 경계**를 학습할 수 있게 해준다.

---

## 12.4 최적화 및 수치적 구현 <a name="12.4"/>

- **What:**  
  SVM의 최적화 문제를 수치적으로 푸는 다양한 알고리즘

- **Why:**  
  정규화, 계산 효율성, 실용성을 위한 구현 기법이 다양함

- **When:**  
  - 실제 학습 시스템 개발  
  - 라이브러리 사용 (LIBSVM 등)  
  - 파라미터 튜닝

- **How:**  
  - 쌍대 문제는 QP(Quadratic Programming)로 변환  
  - 슬랙 변수 및 C 파라미터로 소프트 마진 처리

> ⚙️ **콜아웃**  
> 실전에서는 SVM 솔버와 **하이퍼파라미터 선택**이 성능에 큰 영향을 준다.

---

## 12.5 확률적 해석과 기타 분류기 비교 <a name="12.5"/>

- **What:**  
  SVM의 결정 함수는 확률이 아닌 점수(score)를 반환함

- **Why:**  
  확률로 해석하고 싶다면 별도 보정 또는 다른 모델 사용 필요

- **When:**  
  - 확률 추정 필요할 때  
  - 로지스틱 회귀, 나이브 베이즈 등과 비교  
  - 베이지안 분류기 고려

- **How:**  
  - SVM 점수를 sigmoid에 넣어 확률 보정 (Platt scaling 등)  
  - 대안 모델: 로지스틱 회귀, 랜덤 포레스트, GPC 등

> 📊 **콜아웃**  
> SVM은 **결정경계에 최적화**된 모델이며, 확률 기반 해석은 후처리가 필요하다.

---

## 12.6 참고자료 (Further Reading) <a name="12.6"/>

- 📘 **서적**
  - *Pattern Recognition and Machine Learning* — Christopher Bishop  
  - *Learning with Kernels* — Bernhard Schölkopf, Alexander Smola

- 🎥 **강의**
  - YouTube: StatQuest "Support Vector Machines"  
  - MIT OCW: SVM & Kernel Methods

---

✅ 이번 장을 끝으로 MML의 **4대 머신러닝 문제 유형**(회귀, 차원축소, 밀도추정, 분류)을 모두 다뤘습니다.  
📚 이후에는 각 문제 유형의 고급 이론과 실제 모델 설계로 나아갈 수 있습니다. 필요한 경우 각 장을 바탕으로 **심화 포스트**나 **응용 프로젝트**를 확장해보세요.
