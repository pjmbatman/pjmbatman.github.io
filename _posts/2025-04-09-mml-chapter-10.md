---
title: "🔻 MML Chapter 10: 차원 축소 (Dimensionality Reduction)"
date: 2025-04-09
tags: [MML, 차원축소, PCA, 특잇값분해, 정보압축, 머신러닝수학]
categories: [Study, Mathematics for Machine Learning]
math: true
---

# 📘 MML Study - Chapter 10: Dimensionality Reduction 요약

> 이 장에서는 고차원 데이터를 보다 이해하고 효율적으로 다루기 위한 수단인 **차원 축소** 기법을 다룬다.  
> 특히 **PCA(주성분 분석)**을 수학적으로 유도하고, 데이터 압축, 노이즈 제거, 시각화 등에 어떻게 적용되는지를 설명한다.

---

## 📚 목차

- 10.1 [문제 정의 및 동기](#10.1)
- 10.2 [최대 분산 관점 (Maximum Variance Perspective)](#10.2)
- 10.3 [정사영 관점 (Projection Perspective)](#10.3)
- 10.4 [PCA 알고리즘 요약 및 구현](#10.4)
- 10.5 [PCA in High-Dimensional Settings](#10.5)
- 10.6 [PCA 응용 사례 (e.g., MNIST)](#10.6)
- 10.7 [확장 및 일반화 (Kernel PCA 등)](#10.7)
- 10.8 [참고자료 (Further Reading)](#10.8)

---

## 10.1 문제 정의 및 동기 <a name="10.1"/>

- **What:**  
  고차원 데이터 $x \in \mathbb{R}^D$를 보다 낮은 차원 $z \in \mathbb{R}^M$으로 변환하는 기법. 정보 손실을 최소화하면서 압축

- **Why:**  
  시각화, 계산 효율성, 노이즈 제거, 과적합 방지 등 머신러닝 전반에 필수

- **When:**  
  - 이미지, 자연어 등 고차원 데이터 처리  
  - 전처리 및 시각화  
  - 특성 선택 및 축소

- **How:**  
  고차원 공간의 데이터가 실제로는 저차원 구조를 가짐 (예: 다수의 축은 상관되어 있음)

> 🎯 **콜아웃**  
> 차원 축소는 **데이터 표현을 더 단순하게 만드는 도구**이며, 데이터 구조의 본질을 드러내는 역할을 한다.

---

## 10.2 최대 분산 관점 (Maximum Variance Perspective) <a name="10.2"/>

- **What:**  
  정보량을 분산으로 해석하고, **데이터의 분산을 최대화**하는 방향으로 투영

- **Why:**  
  많은 정보를 담은 축 = 분산이 큰 방향

- **How:**  
  - 공분산 행렬 $S = \frac{1}{N} \sum x_n x_n^\top$ 계산  
  - 고유값 분해로 큰 고유값 방향을 선택  
  - $B = [b_1, ..., b_M]$, $z_n = B^\top x_n$

> 📐 **콜아웃**  
> PCA는 분산을 가장 잘 **보존하는 M차원 서브스페이스**를 찾는 문제로 귀결된다

---

## 10.3 정사영 관점 (Projection Perspective) <a name="10.3"/>

- **What:**  
  데이터 $x$를 어떤 저차원 공간 $U$로 **직교 정사영**한 후, 복원오차 $\|x - \tilde{x}\|^2$를 최소화하는 방식으로 유도

- **Why:**  
  재구성 가능한 정보 유지량을 수학적으로 명확히 정의 가능

- **How:**  
  - $x̃ = BB^\top x$  
  - $z = B^\top x$  
  - 주성분: 공분산 행렬의 고유벡터

> 🧮 **콜아웃**  
> PCA는 **선형 오토인코더(linear autoencoder)**와 동일한 수학 구조를 갖는다

---

## 10.4 PCA 알고리즘 요약 및 구현 <a name="10.4"/>

1. 데이터 정규화 (평균 0)
2. 공분산 행렬 계산
3. 고유값 분해 또는 SVD 수행
4. 상위 M개의 고유벡터 선택 → 주성분
5. 변환: $z_n = B^\top x_n$

> 💡 **콜아웃**  
> SVD를 사용하면 PCA를 보다 **안정적이고 효율적**으로 구현 가능

---

## 10.5 PCA in High-Dimensional Settings <a name="10.5"/>

- $D \gg N$인 경우, $X^\top X$ 기반으로 PCA 계산  
- 계산량 감소, 특히 이미지 데이터나 유전체 데이터 등에서 중요

> ⚙️ **콜아웃**  
> 샘플 수보다 차원이 큰 경우, **“dual PCA” 또는 “kernel PCA”**로 접근할 수 있음

---

## 10.6 PCA 응용 사례: MNIST 복원 <a name="10.6"/>

- 입력: 28×28 이미지 (784차원)  
- PCA로 압축한 후 재구성 비교 (1, 10, 100, 500 PCs)

> 🧩 **콜아웃**  
> 소수의 주성분만으로도 **형태 유지**, 많은 차원은 **노이즈**였음을 시사함

---

## 10.7 확장 및 일반화 (Kernel PCA 등) <a name="10.7"/>

- 커널 PCA: 고차원 특징공간에서 PCA 수행  
- ICA, LLE, Isomap 등 다양한 비선형 차원 축소 기법 존재  
- Autoencoder로 일반화 가능 (비선형 매핑 학습)

> 🔍 **콜아웃**  
> 선형 PCA는 시작일 뿐. **비선형 구조**를 반영하려면 Autoencoder나 GP-LVM 등을 활용해야 한다

---

## 10.8 참고자료 (Further Reading) <a name="10.8"/>

- 📘 **서적**
  - *Pattern Recognition and Machine Learning* — C. Bishop  
  - *Elements of Statistical Learning* — Hastie et al.  

- 🎥 **강의**
  - CMU 10-701 Lecture: PCA & SVD  
  - YouTube: StatQuest "PCA Explained"

---

✅ 다음 장인 **[Chapter 11: 밀도 추정 (Density Estimation)]**에서는 데이터를 생성하는 **확률 분포 자체를 추정**하는 과정을 다룬다.  
📊 GMM, EM 알고리즘, 잠재 변수 모델 등을 통해 **모델링의 또 다른 관점**을 배워보자.
