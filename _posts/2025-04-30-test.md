---
title: "📐 MML Chapter 2: 선형대수학 (Linear Algebra)"
date: 2025-05-01
tags: [MML, 선형대수, Linear Algebra, 머신러닝수학]
math: true
---

# 📘 MML Study - Chapter 2: Linear Algebra 요약

> 이 장에서는 머신러닝의 기초가 되는 **벡터와 행렬, 선형 방정식, 벡터공간과 선형사상**을 다룹니다.  
> 모든 머신러닝 알고리즘은 결국 벡터와 행렬 위에서 작동하며, 이 장은 그런 이론적 기반을 제공합니다.

---

## 📚 목차 (Index)

- 2.1 [선형연립방정식 (Systems of Linear Equations)](#2.1)
- 2.2 [행렬 (Matrices)](#2.2)
- 2.3 [선형방정식의 해법 (Solving Systems of Linear Equations)](#2.3)
- 2.4 [벡터공간 (Vector Spaces)](#2.4)
- 2.5 [선형독립성 (Linear Independence)](#2.5)
- 2.6 [기저와 랭크 (Basis and Rank)](#2.6)
- 2.7 [선형사상 (Linear Mappings)](#2.7)
- 2.8 [어파인 공간 (Affine Spaces)](#2.8)
- 2.9 [참고자료 (Further Reading)](#2.9)

---

## 2.1 선형연립방정식 (Systems of Linear Equations) <a name="2.1"/>

### 🔹 What
$Ax = b$ 형태의 방정식 집합으로, 여러 변수에 대한 선형 조건을 만족하는 해를 구하는 문제입니다.

### ❓ Why
데이터 fitting, 예측 모델 학습, 최적화 문제 등에서 파라미터를 찾기 위한 기본 수단입니다.

### 📍 When
- 선형 회귀의 해 도출  
- 정사영 해석 (Chapter 9, 10)  
- 신경망의 가중치 해 (초기 버전)

### ⚙️ How
- 형태: $A \in \mathbb{R}^{m \times n}$, $x \in \mathbb{R}^n$, $b \in \mathbb{R}^m$  
- 해의 개수:
  - 무해 (no solution): 평면들이 교차하지 않음
  - 유일 해 (unique): 한 점에서 교차
  - 무한 해 (infinitely many): 평면이 겹침

---

## 2.2 행렬 (Matrices) <a name="2.2"/>

### 🔹 What
행렬은 벡터들의 집합 혹은 선형사상의 표현 도구로, 연립방정식과 데이터 구조를 표현하는 핵심입니다.

### ❓ Why
모든 머신러닝 모델은 행렬 연산 위에서 작동하며, 학습은 행렬 연산의 조합으로 이뤄집니다.

### 📍 When
- 입력 데이터를 표현할 때  
- 모델 파라미터 (가중치 등)를 표현할 때  
- 선형 변환 계산 시

### ⚙️ How
- 기본 연산: 덧셈, 스칼라 곱, 행렬곱 ($AB$), 전치($A^\top$), 역행렬($A^{-1}$)
- 단위행렬: $I_n$은 항등 연산
- 곱셈 정의:
  $$
  (AB)_{ij} = \sum_k A_{ik} B_{kj}
  $$

---

## 2.3 선형방정식의 해법 (Solving Systems of Linear Equations) <a name="2.3"/>

### 🔹 What
선형 연립방정식의 해를 특해 + 일반해로 구분하고, 이를 가우스 소거법으로 계산합니다.

### ❓ Why
대부분의 머신러닝 모델 학습은 해를 갖는 최적화 문제로 귀결됩니다.

### 📍 When
- $Ax = b$ 해 찾기
- 최소제곱해 구하기
- 역행렬 계산

### ⚙️ How
- 가우스 소거법으로 REF (Row Echelon Form) 또는 RREF (Reduced REF) 생성
- 특해: $Ax = b$의 임의 해
- 일반해: $Ax = 0$의 모든 해
- 해 = 특해 + 영공간 (kernel)

---

## 2.4 벡터공간 (Vector Spaces) <a name="2.4"/>

### 🔹 What
덧셈과 스칼라곱에 대해 닫혀 있는 벡터들의 집합입니다.

### ❓ Why
모든 데이터, 함수, 매핑을 벡터공간으로 해석해야 수학적으로 조작 가능해집니다.

### 📍 When
- 차원축소 (PCA)  
- 특징 표현  
- 임베딩 공간 설계

### ⚙️ How
- 예: $\mathbb{R}^n$, $\mathbb{R}^{m \times n}$, 다항식, 함수공간 등
- 부분공간 (Subspace): $U \subseteq V$, $0 \in U$, 선형결합에 대해 닫힘

---

## 2.5 선형독립성 (Linear Independence) <a name="2.5"/>

### 🔹 What
벡터들이 서로 다른 방향을 갖고 있어 중복이 없는 성질

### ❓ Why
기저와 차원을 결정짓고, 모델의 일반화 및 중복제거에서 핵심

### 📍 When
- 차원 축소 시 중요  
- 고유 벡터 추출  
- 신경망 중복 가중치 제거

### ⚙️ How
- $\sum_i \lambda_i x_i = 0$이 유일 해를 갖는다면 선형독립
- 가우스 소거법으로 피벗 개수 확인

---

## 2.6 기저와 랭크 (Basis and Rank) <a name="2.6"/>

### 🔹 What
기저는 벡터공간을 생성하는 최소 집합, 랭크는 선형독립 열/행의 개수

### ❓ Why
데이터 표현의 최소 단위이며, 차원을 줄이거나 구조를 파악하는 데 중요

### 📍 When
- PCA에서 주요 성분 추출  
- 데이터 압축  
- 선형모델의 해석

### ⚙️ How
- 기저는 선형독립이면서 전체 공간을 생성하는 벡터 집합
- Rank–Nullity 정리:
  $$
  \text{dim}(\ker A) + \text{rank}(A) = \text{dim}(V)
  $$

---

## 2.7 선형사상 (Linear Mappings) <a name="2.7"/>

### 🔹 What
벡터 공간을 다른 벡터 공간으로 선형 변환하는 함수. 행렬로 표현 가능.

### ❓ Why
모든 모델은 입력 데이터를 특정 출력 공간으로 사상(mapping)합니다.

### 📍 When
- 뉴럴넷 계층  
- PCA 투영  
- 고차원 임베딩

### ⚙️ How
- $f(x + y) = f(x) + f(y)$, $f(\lambda x) = \lambda f(x)$
- 행렬 표현: $y = A x$
- 기저변환: $A' = T^{-1} A S$
- 핵심 구조:
  - 커널 (Null Space): $Ax = 0$의 해 공간
  - 이미지 (Image): $Ax$가 생성하는 공간

---

## 2.8 어파인 공간 (Affine Spaces) <a name="2.8"/>

### 🔹 What
벡터공간에서 원점을 이동시킨 공간으로, 선형공간이 아닌 '선형+이동' 구조

### ❓ Why
SVM, 회귀 등은 원점을 지나지 않는 **하이퍼플레인**을 사용합니다.

### 📍 When
- 하이퍼플레인 정의  
- 바이어스 있는 모델 (e.g., $y = Wx + b$)  
- 공간 이동을 포함하는 표현

### ⚙️ How
- 표현: $L = x_0 + U$, $x_0$은 지지점, $U$는 방향 부분공간
- 어파인 사상: $ϕ(x) = Ax + b$ (선형변환 + 평행이동)

---

## 2.9 참고자료 (Further Reading) <a name="2.9"/>

- 📘 책
  - *Linear Algebra Done Right* — Sheldon Axler
  - *Introduction to Linear Algebra* — Gilbert Strang
- 🎥 영상
  - MIT OpenCourseWare: Strang 강의
  - 3Blue1Brown: Essence of Linear Algebra

---

✅ 다음은 [Chapter 3: 해석기하학 (Analytic Geometry)]에서 **벡터 간의 거리, 각도, 정사영** 등의 개념을 도입합니다.  
📐 **수학적 구조 + 기하적 직관**을 함께 다져볼 시간입니다.

