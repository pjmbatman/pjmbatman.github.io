---
title: "📐 MML Chapter 2: 선형대수학 (Linear Algebra)"
date: 2025-05-01
tags: [MML, 선형대수, Linear Algebra, 머신러닝수학]
math: true
---

# 📘 MML Study - Chapter 2: Linear Algebra 요약

> 본 장에서는 머신러닝 이론의 기반을 이루는 **벡터와 행렬, 선형 방정식, 벡터공간과 선형사상**에 대해 다룬다.  
> 대부분의 머신러닝 모델은 벡터 및 행렬 연산을 바탕으로 정의되며, 따라서 이 장은 실질적인 모델 학습 및 추론에 앞서 반드시 이해해야 할 수학적 기반을 제공한다.

---

## 📚 목차

- 2.1 [선형연립방정식 (Systems of Linear Equations)](#2.1)
- 2.2 [행렬 (Matrices)](#2.2)
- 2.3 [선형방정식의 해법 (Solving Systems of Linear Equations)](#2.3)
- 2.4 [벡터공간 (Vector Spaces)](#2.4)
- 2.5 [선형독립성 (Linear Independence)](#2.5)
- 2.6 [기저와 랭크 (Basis and Rank)](#2.6)
- 2.7 [선형사상 (Linear Mappings)](#2.7)
- 2.8 [어파인 공간 (Affine Spaces)](#2.8)
- 2.9 [참고자료 (Further Reading)](#2.9)

---

## 2.1 선형연립방정식 (Systems of Linear Equations) <a name="2.1"/>

- **정의 (What):**  
  여러 개의 선형 방정식을 집합으로 구성한 형태로, 보통 $Ax = b$로 표현된다. 여기서 $A \in \mathbb{R}^{m \times n}$는 계수 행렬, $x \in \mathbb{R}^n$는 해 변수 벡터, $b \in \mathbb{R}^m$는 상수 벡터이다.

- **의의 (Why):**  
  모델의 학습, 회귀 계수 추정, 최적화 해 도출 등에서 핵심적인 도구이다.

- **활용 시점 (When):**  
  - 선형 회귀 해 구할 때  
  - 정사영 (projection) 해석 시  
  - 간단한 신경망의 가중치 계산 시

- **방법 (How):**  
  해의 개수는 다음 세 가지 경우 중 하나이다:
  - 해 없음 (no solution) — 평면들이 교차하지 않음  
  - 유일 해 (unique solution) — 한 점에서 교차  
  - 무한 해 (infinite solutions) — 평면이 동일  

> 💡 **콜아웃**  
> 선형 연립방정식은 머신러닝에서 가장 기본적이며 빈번하게 등장하는 수학적 문제다. 이는 모델 파라미터 추정, 회귀 해석, 예측 등 핵심 모듈을 구성하는 수단이다.

---

## 2.2 행렬 (Matrices) <a name="2.2"/>

- **정의 (What):**  
  행렬은 수치를 직사각형 형태로 배열한 것으로, 연립방정식, 데이터 구조, 선형사상을 표현하는 기본 단위이다.

- **의의 (Why):**  
  머신러닝의 모든 연산은 결국 행렬의 덧셈, 곱셈, 전치 등을 반복 적용하여 이루어진다.

- **활용 시점 (When):**  
  - 입력 데이터 표현  
  - 모델 파라미터 구성  
  - 선형 변환 수행

- **방법 (How):**  
  주요 연산은 다음과 같다:
  - 행렬 곱: $(AB)_{ij} = \sum_k A_{ik} B_{kj}$
  - 전치: $A^\top$
  - 역행렬: $A^{-1}$  
  - 항등 행렬: $I_n$  

> 🧠 **콜아웃**  
> 행렬은 단순한 숫자의 배열이 아니라 '연산의 규칙'을 내포하는 연산자이기도 하다. 특히 선형사상의 표현 수단으로서 중요하다.

---

## 2.3 선형방정식의 해법 (Solving Systems of Linear Equations) <a name="2.3"/>

- **정의 (What):**  
  $Ax = b$ 형태의 문제에서 특해와 일반해를 구하는 것. 이 때 가우스 소거법을 통해 행 사다리꼴(RREF) 형태로 변형한다.

- **의의 (Why):**  
  대부분의 머신러닝 문제는 어떤 형태로든 이 방정식의 해를 구하는 문제로 환원된다.

- **활용 시점 (When):**  
  - 선형 회귀 해 구하기  
  - 최소제곱 해법  
  - 역행렬 계산

- **방법 (How):**  
  - 가우스 소거법으로 RREF 생성  
  - 해는 특해 + 영공간(Null space)의 조합으로 구성됨  

> 📌 **콜아웃**  
> 시스템이 너무 크거나 조건이 부적절한 경우 직접적인 해보다는 **근사해 (approximate solution)**가 유용할 수 있음. PCA, 회귀 등에서 중요.

---

## 2.4 벡터공간 (Vector Spaces) <a name="2.4"/>

- **정의 (What):**  
  벡터의 덧셈과 스칼라 곱 연산에 대해 닫혀 있는 집합

- **의의 (Why):**  
  모델의 파라미터, 데이터, 함수 등은 모두 벡터공간 상에서 정의되므로, 이를 이해하면 다양한 수학적 조작이 가능해진다.

- **활용 시점 (When):**  
  - 차원 축소 (PCA)  
  - 표현 학습  
  - 임베딩 구조 설계

- **방법 (How):**  
  예시:
  - $\mathbb{R}^n$, $\mathbb{R}^{m \times n}$  
  - 함수 공간, 다항식 공간 등  
  - 부분공간 조건: $0 \in U$, 선형결합에 대해 닫힘

> 🎯 **콜아웃**  
> 벡터공간 개념 없이는 정사영, 기저, 차원 축소 등의 수학적 개념을 해석할 수 없음.

---

## 2.5 선형독립성 (Linear Independence) <a name="2.5"/>

- **정의 (What):**  
  벡터들이 서로를 선형결합으로 표현할 수 없다면 선형독립이다.

- **의의 (Why):**  
  중복 없는 정보 집합을 구성하고, 차원을 정의하는 데 필수적이다.

- **활용 시점 (When):**  
  - 차원 축소 및 PCA  
  - 중복 제거  
  - 신경망 가중치 최적화

- **방법 (How):**  
  - $\sum_i \lambda_i x_i = 0$에서 유일한 해가 $\lambda_i = 0$인 경우  
  - 가우스 소거법에서 피벗 개수 확인

> 🔍 **콜아웃**  
> 독립성과 기저는 항상 짝을 이룬다. 기저는 '최소한의 선형독립한 생성 집합'이다.

---

## 2.6 기저와 랭크 (Basis and Rank) <a name="2.6"/>

- **정의 (What):**  
  기저는 벡터공간을 생성하는 선형독립 벡터들의 집합이며, 랭크는 선형독립 행/열의 수이다.

- **의의 (Why):**  
  데이터 압축, 정보 표현의 최소 단위로 중요하며, 구조를 파악하는 데 필수적이다.

- **활용 시점 (When):**  
  - 차원 축소 (PCA)  
  - 모델 단순화 및 해석  
  - 데이터 압축

- **방법 (How):**  
  - 기저: 선형독립 + 전체 공간을 생성  
  - 랭크-널리티 정리:
    $$
    \text{dim}(\ker A) + \text{rank}(A) = \text{dim}(V)
    $$

> 📏 **콜아웃**  
> 랭크는 행렬의 '정보 용량'을 측정하는 척도이다.

---

## 2.7 선형사상 (Linear Mappings) <a name="2.7"/>

- **정의 (What):**  
  벡터 공간에서 다른 벡터 공간으로의 선형 변환 함수이며, 행렬로 표현 가능하다.

- **의의 (Why):**  
  모든 모델은 입력을 출력으로 변환하는 사상을 포함한다.

- **활용 시점 (When):**  
  - 뉴럴넷 계층 변환  
  - PCA 투영  
  - 고차원 임베딩

- **방법 (How):**  
  - $f(x + y) = f(x) + f(y)$, $f(\lambda x) = \lambda f(x)$  
  - 행렬 표현: $y = A x$  
  - 핵심 구조:
    - 커널 (Null space): $Ax = 0$  
    - 이미지 (Image): $Ax$가 생성하는 공간

> 🔄 **콜아웃**  
> 선형사상은 단순 연산의 반복이 아니라, **공간 구조를 보존하는 변환**이다.

---

## 2.8 어파인 공간 (Affine Spaces) <a name="2.8"/>

- **정의 (What):**  
  벡터공간에서 원점을 이동시킨 형태. 벡터공간이 아닌, "선형 + 이동" 구조를 가진다.

- **의의 (Why):**  
  회귀, SVM 등에서 바이어스 항을 포함한 하이퍼플레인을 정의할 때 필요하다.

- **활용 시점 (When):**  
  - 하이퍼플레인 정의  
  - 바이어스 포함한 모델 구조 (e.g., $y = Wx + b$)  
  - 좌표 이동 표현

- **방법 (How):**  
  - 어파인 공간: $L = x_0 + U$, $x_0$는 기준점, $U$는 방향 부분공간  
  - 어파인 사상: $\phi(x) = Ax + b$

> 🧭 **콜아웃**  
> 현실의 데이터는 대부분 원점을 지나지 않기 때문에 어파인 공간 개념이 중요하다.

---

## 2.9 참고자료 (Further Reading) <a name="2.9"/>

- 📘 **서적**
  - *Linear Algebra Done Right* — Sheldon Axler  
  - *Introduction to Linear Algebra* — Gilbert Strang  

- 🎥 **강의/영상**
  - MIT OCW - Strang 교수 강의  
  - YouTube: 3Blue1Brown "Essence of Linear Algebra" 시리즈

---

✅ 다음 장인 **[Chapter 3: 해석기하학 (Analytic Geometry)]**에서는 벡터 간 거리, 각도, 정사영 등의 개념을 기반으로 **기하학적 직관**을 수학적으로 통합해 나갈 예정이다.
